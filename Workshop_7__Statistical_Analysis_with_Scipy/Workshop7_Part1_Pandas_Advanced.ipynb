{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nepal-Research-and-Collaboration-Center/RT2024/blob/main/Workshop_7__Statistical_Analysis_with_Scipy/Workshop7_Part1_Pandas_Advanced.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop 7: Data Wrangling with Pandas and Statistical Analysis with Scipy\n",
    "\n",
    "### Part 1: Data Wrangling with Pandas\n",
    "\n",
    "In this section, we will cover the basics of data wrangling using Pandas. We'll start by understanding what data wrangling is and why it's essential, followed by advanced Pandas techniques, and finally, we'll learn how to merge and join DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Data Wrangling with Pandas\n",
    "\n",
    "**Data Wrangling** is the process of cleaning, transforming, and organizing raw data into a desired format for analysis. It is a crucial step in the data analysis pipeline as it ensures that the data is consistent, accurate, and ready for analysis.\n",
    "\n",
    "### Why is Data Wrangling Important?\n",
    "\n",
    "- **Improves Data Quality:** Ensures that the data is free from errors and inconsistencies.\n",
    "- **Facilitates Analysis:** Makes the data easier to work with and analyze.\n",
    "- **Enables Better Decision Making:** Leads to more accurate and reliable insights from the data.\n",
    "\n",
    "### Example: \n",
    "Consider a dataset of air quality measurements in Kathmandu. The data might contain missing values, duplicates, or inconsistencies in format. Data wrangling would involve cleaning this data to make it suitable for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a dataset containing air quality measurements (e.g., PM2.5 levels) collected from various stations in Kathmandu over a week. However, due to sensor issues or data transmission problems, some data points are missing, and there are duplicate entries. Your task is to clean the dataset by handling the missing values and removing duplicates.\n",
    "\n",
    "Here’s how you can approach this using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "         Date  Station 1  Station 2  Station 3\n",
      "0  2024-08-01       55.0       50.0        NaN\n",
      "1  2024-08-02       60.0       52.0       48.0\n",
      "2  2024-08-02        NaN       54.0       50.0\n",
      "3  2024-08-03       65.0        NaN       52.0\n",
      "4         NaN       70.0       58.0        NaN\n",
      "5  2024-08-05        NaN       60.0       56.0\n",
      "6  2024-08-05       75.0        NaN       58.0\n",
      "7  2024-08-06       80.0       64.0       60.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset: Air quality measurements from three different stations over a week\n",
    "data = {\n",
    "    'Date': ['2024-08-01', '2024-08-02', '2024-08-02', '2024-08-03', np.nan, '2024-08-05', '2024-08-05', '2024-08-06'],\n",
    "    'Station 1': [55, 60, np.nan, 65, 70, np.nan, 75, 80],\n",
    "    'Station 2': [50, 52, 54, np.nan, 58, 60, np.nan, 64],\n",
    "    'Station 3': [np.nan, 48, 50, 52, np.nan, 56, 58, 60]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Missing Data\n",
    "\n",
    "We will handle missing data by using different techniques, such as filling missing values with forward fill or using interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset After Forward Filling Missing Values:\n",
      "         Date  Station 1  Station 2  Station 3\n",
      "0  2024-08-01       55.0       50.0        NaN\n",
      "1  2024-08-02       60.0       52.0       48.0\n",
      "2  2024-08-02       60.0       54.0       50.0\n",
      "3  2024-08-03       65.0       54.0       52.0\n",
      "4  2024-08-03       70.0       58.0       52.0\n",
      "5  2024-08-05       70.0       60.0       56.0\n",
      "6  2024-08-05       75.0       60.0       58.0\n",
      "7  2024-08-06       80.0       64.0       60.0\n",
      "\n",
      "Dataset After Interpolating Missing Values:\n",
      "         Date  Station 1  Station 2  Station 3\n",
      "0  2024-08-01       55.0       50.0        NaN\n",
      "1  2024-08-02       60.0       52.0       48.0\n",
      "2  2024-08-02       62.5       54.0       50.0\n",
      "3  2024-08-03       65.0       56.0       52.0\n",
      "4         NaN       70.0       58.0       54.0\n",
      "5  2024-08-05       72.5       60.0       56.0\n",
      "6  2024-08-05       75.0       62.0       58.0\n",
      "7  2024-08-06       80.0       64.0       60.0\n"
     ]
    }
   ],
   "source": [
    "# Forward fill missing values for each station\n",
    "df_filled = df.fillna(method='ffill')\n",
    "\n",
    "# Alternatively, you can use interpolation to estimate missing values\n",
    "df_interpolated = df.interpolate()\n",
    "\n",
    "print(\"\\nDataset After Forward Filling Missing Values:\")\n",
    "print(df_filled)\n",
    "\n",
    "print(\"\\nDataset After Interpolating Missing Values:\")\n",
    "print(df_interpolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Duplicate Entries\n",
    "\n",
    "Next, we will remove any duplicate entries in the dataset, particularly on the ‘Date’ column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset After Removing Duplicates:\n",
      "         Date  Station 1  Station 2  Station 3\n",
      "0  2024-08-01       55.0       50.0        NaN\n",
      "1  2024-08-02       60.0       52.0       48.0\n",
      "3  2024-08-03       65.0        NaN       52.0\n",
      "4         NaN       70.0       58.0        NaN\n",
      "5  2024-08-05        NaN       60.0       56.0\n",
      "7  2024-08-06       80.0       64.0       60.0\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on the 'Date' column\n",
    "df_no_duplicates = df.drop_duplicates(subset='Date')\n",
    "\n",
    "print(\"\\nDataset After Removing Duplicates:\")\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Pandas Techniques\n",
    "\n",
    "In this section, we will explore some advanced techniques for data wrangling using Pandas.\n",
    "\n",
    "### 2.1 Data Cleaning\n",
    "- **Handling Missing Data:**\n",
    "  - `dropna()`: Remove missing values.\n",
    "  - `fillna()`: Fill missing values with a specified value or method.\n",
    "  - `interpolate()`: Fill missing data with interpolation.\n",
    "  \n",
    "- **Removing Duplicates:**\n",
    "  - `drop_duplicates()`: Remove duplicate rows from the DataFrame.\n",
    "\n",
    "- **Data Type Conversions:**\n",
    "  - `astype()`: Convert data types of columns.\n",
    "  - Parsing dates with `pd.to_datetime()`.\n",
    "\n",
    "### Example:\n",
    "Let's clean a dataset containing missing and duplicate data, and ensure all columns have the correct data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "         Date  Temperature  Humidity\n",
      "0  2024-08-01         25.0      60.0\n",
      "1  2024-08-02          NaN      65.0\n",
      "2         NaN         23.0      63.0\n",
      "3  2024-08-04         22.0       NaN\n",
      "4  2024-08-04         22.0      63.0\n",
      "\n",
      "Cleaned Dataset (Missing data handled, duplicates removed, and date parsed):\n",
      "         Date  Temperature  Humidity\n",
      "0  2024-08-01         25.0      60.0\n",
      "1  2024-08-02         25.0      65.0\n",
      "2  2024-08-02         23.0      63.0\n",
      "3  2024-08-04         22.0      63.0\n",
      "4  2024-08-04         22.0      63.0\n"
     ]
    }
   ],
   "source": [
    "# Example dataset\n",
    "data = {\n",
    "    'Date': ['2024-08-01', '2024-08-02', np.nan, '2024-08-04', '2024-08-04'],\n",
    "    'Temperature': [25, np.nan, 23, 22, 22],\n",
    "    'Humidity': [60, 65, 63, np.nan, 63]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Handling missing data\n",
    "df_cleaned = df.dropna()  # Drop rows with missing values\n",
    "df_filled = df.fillna(method='ffill')  # Forward fill missing values\n",
    "\n",
    "# Removing duplicates\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "\n",
    "# Data type conversion\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(\"\\nCleaned Dataset (Missing data handled, duplicates removed, and date parsed):\")\n",
    "print(df_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas `fillna()` Method\n",
    "\n",
    "The `fillna()` method in Pandas is used to fill missing (NaN) values in a DataFrame or Series. This method provides various ways to handle missing data, making it a versatile tool for data cleaning and preprocessing.\n",
    "\n",
    "1. **`value`:**  \n",
    "   Fills NaN values with a specified value.\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     df.fillna(0)  # Fills all NaN values with 0\n",
    "     ```\n",
    "   - You can also use a dictionary to specify different fill values for different columns.\n",
    "     ```python\n",
    "     df.fillna({'col1': 0, 'col2': 1})\n",
    "     ```\n",
    "\n",
    "2. **`method`:**  \n",
    "   Fills NaN values using a specified method.\n",
    "   - **Options:**\n",
    "     - `'ffill'` (Forward Fill): Propagates the last valid observation forward.\n",
    "       ```python\n",
    "       df.fillna(method='ffill')  # Forward fill NaN values\n",
    "       ```\n",
    "     - `'bfill'` (Backward Fill): Propagates the next valid observation backward.\n",
    "       ```python\n",
    "       df.fillna(method='bfill')  # Backward fill NaN values\n",
    "       ```\n",
    "\n",
    "3. **`axis`:**  \n",
    "   Specifies the axis along which to fill NaN values.\n",
    "   - **Default:** `axis=0` (Fills NaN values column-wise).\n",
    "   - **Alternative:**\n",
    "     - `axis=1`: Fills NaN values row-wise.\n",
    "       ```python\n",
    "       df.fillna(method='ffill', axis=1)  # Forward fill NaN values along rows\n",
    "       ```\n",
    "\n",
    "4. **`limit`:**  \n",
    "   Limits the maximum number of NaN values to fill.\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     df.fillna(method='ffill', limit=1)  # Forward fill only one NaN value per column\n",
    "     ```\n",
    "\n",
    "5. **`inplace`:**  \n",
    "   If set to `True`, modifies the original DataFrame/Series by filling NaN values in place. If `False`, returns a new DataFrame/Series with the filled values.\n",
    "   - **Default:** `False`.\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     df.fillna(0, inplace=True)  # Fill NaN values with 0 and modify the original DataFrame\n",
    "     ```\n",
    "\n",
    "6. **`downcast`:**  \n",
    "   Allows downcasting the resulting DataFrame/Series to a more specific data type if possible (e.g., from `float64` to `int32`).\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     df.fillna(0, downcast='infer')  # Fill NaN values with 0 and downcast if possible\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the duplicate records \n",
    "Imagine you have a DataFrame containing data on air quality measurements from various monitoring stations in Kathmandu. Due to data collection issues, some records are duplicated. You want to clean your dataset by removing these duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset with Duplicates:\n",
      "         Date    Station  PM2.5\n",
      "0  2024-08-01  Station 1     55\n",
      "1  2024-08-01  Station 1     55\n",
      "2  2024-08-02  Station 2     60\n",
      "3  2024-08-03  Station 3     65\n",
      "4  2024-08-03  Station 3     65\n",
      "5  2024-08-04  Station 3     70\n",
      "\n",
      "Dataset after Removing Duplicates (All Columns):\n",
      "         Date    Station  PM2.5\n",
      "0  2024-08-01  Station 1     55\n",
      "2  2024-08-02  Station 2     60\n",
      "3  2024-08-03  Station 3     65\n",
      "5  2024-08-04  Station 3     70\n",
      "\n",
      "Dataset after Removing Duplicates (Specific Columns - 'Station'):\n",
      "         Date    Station  PM2.5\n",
      "0  2024-08-01  Station 1     55\n",
      "2  2024-08-02  Station 2     60\n",
      "3  2024-08-03  Station 3     65\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset with duplicate rows\n",
    "data = {\n",
    "    'Date': ['2024-08-01', '2024-08-01', '2024-08-02', '2024-08-03', '2024-08-03', '2024-08-04'],\n",
    "    'Station': ['Station 1', 'Station 1', 'Station 2', 'Station 3', 'Station 3', 'Station 3'],\n",
    "    'PM2.5': [55, 55, 60, 65, 65, 70]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset with Duplicates:\")\n",
    "print(df)\n",
    "\n",
    "# Remove duplicate rows based on all columns\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "\n",
    "# Alternatively, remove duplicates based on the 'Station' columns only\n",
    "df_no_duplicates_specific = df.drop_duplicates(subset=[ 'Station'])\n",
    "\n",
    "# Display the cleaned dataset without duplicates\n",
    "print(\"\\nDataset after Removing Duplicates (All Columns):\")\n",
    "print(df_no_duplicates)\n",
    "\n",
    "print(\"\\nDataset after Removing Duplicates (Specific Columns - 'Station'):\")\n",
    "print(df_no_duplicates_specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 3: Merging and Joining DataFrames\n",
    "\n",
    "Combining data from multiple sources is a common task in data analysis. Pandas provides powerful tools to merge and join DataFrames, enabling you to integrate related datasets efficiently.\n",
    "\n",
    "### 3.1 Concatenation\n",
    "`pd.concat()` is used to concatenate DataFrames either vertically (adding rows) or horizontally (adding columns). This method is useful when you have DataFrames that share the same columns and you want to stack them, or when you want to add more columns to an existing DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical Concatenation:\n",
      "         Date  PM2.5_Station1\n",
      "0  2024-08-01              55\n",
      "1  2024-08-02              60\n",
      "2  2024-08-03              65\n",
      "0  2024-08-04              70\n",
      "1  2024-08-05              75\n",
      "2  2024-08-06              80\n",
      "\n",
      "Horizontal Concatenation:\n",
      "         Date  PM2.5_Station1        Date  PM2.5_Station1\n",
      "0  2024-08-01              55  2024-08-04              70\n",
      "1  2024-08-02              60  2024-08-05              75\n",
      "2  2024-08-03              65  2024-08-06              80\n"
     ]
    }
   ],
   "source": [
    "# DataFrames to concatenate\n",
    "df1 = pd.DataFrame({\n",
    "    'Date': ['2024-08-01', '2024-08-02', '2024-08-03'],\n",
    "    'PM2.5_Station1': [55, 60, 65]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'Date': ['2024-08-04', '2024-08-05', '2024-08-06'],\n",
    "    'PM2.5_Station1': [70, 75, 80]\n",
    "})\n",
    "\n",
    "# Vertical concatenation (stacking rows)\n",
    "df_vertical = pd.concat([df1, df2])\n",
    "\n",
    "# Horizontal concatenation (adding columns)\n",
    "df_horizontal = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# Display results\n",
    "print(\"Vertical Concatenation:\")\n",
    "print(df_vertical)\n",
    "\n",
    "print(\"\\nHorizontal Concatenation:\")\n",
    "print(df_horizontal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Merging\n",
    "`pd.merge()` allows you to merge two DataFrames based on common columns or indices, similar to SQL joins. You can perform different types of joins, including inner, outer, left, and right joins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Join:\n",
      "         Date  PM2.5_Station1  PM2.5_Station2\n",
      "0  2024-08-05              75              50\n",
      "1  2024-08-06              80              58\n",
      "2  2024-08-07              85              62\n",
      "3  2024-08-08              90              68\n",
      "4  2024-08-09              95              72\n",
      "5  2024-08-10             100              76\n",
      "6  2024-08-11             105              80\n",
      "7  2024-08-12             110              84\n",
      "8  2024-08-13             115              88\n",
      "9  2024-08-14             120              92\n",
      "10 2024-08-15             125              96\n"
     ]
    }
   ],
   "source": [
    "# DataFrames for merging\n",
    "data_station1 = {\n",
    "    'Date': pd.date_range(start='2024-08-01', periods=15, freq='D'),\n",
    "    'PM2.5_Station1': [55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125]\n",
    "}\n",
    "\n",
    "data_station2 = {\n",
    "    'Date': pd.date_range(start='2024-08-05', periods=15, freq='D'),\n",
    "    'PM2.5_Station2': [50, 58, 62, 68, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112]\n",
    "}\n",
    "\n",
    "df_station1 = pd.DataFrame(data_station1)\n",
    "df_station2 = pd.DataFrame(data_station2)\n",
    "\n",
    "# Inner Join (default)\n",
    "df_inner = pd.merge(df_station1, df_station2, on='Date')\n",
    "\n",
    "print(\"Inner Join:\")\n",
    "print(df_inner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Joining on Indices\n",
    "`DataFrame.join()` is similar to `merge()`, but it is used primarily to join DataFrames based on their indices. This is particularly useful when working with DataFrames that share the same index structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames Joined on Index:\n",
      "            PM2.5_Station1  PM2.5_Station2\n",
      "2024-08-01              55              50\n",
      "2024-08-02              60              58\n",
      "2024-08-03              65              62\n",
      "2024-08-04              70              68\n",
      "2024-08-05              75              72\n",
      "2024-08-06              80              76\n",
      "2024-08-07              85              80\n",
      "2024-08-08              90              84\n",
      "2024-08-09              95              88\n",
      "2024-08-10             100              92\n",
      "2024-08-11             105              96\n",
      "2024-08-12             110             100\n",
      "2024-08-13             115             104\n",
      "2024-08-14             120             108\n",
      "2024-08-15             125             112\n"
     ]
    }
   ],
   "source": [
    "# Create larger DataFrames with a common index\n",
    "df_station1 = pd.DataFrame({\n",
    "    'PM2.5_Station1': [55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125]\n",
    "}, index=pd.date_range(start='2024-08-01', periods=15, freq='D'))\n",
    "\n",
    "df_station2 = pd.DataFrame({\n",
    "    'PM2.5_Station2': [50, 58, 62, 68, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112]\n",
    "}, index=pd.date_range(start='2024-08-01', periods=15, freq='D'))\n",
    "\n",
    "# Join DataFrames on their indices\n",
    "df_joined = df_station1.join(df_station2)\n",
    "\n",
    "# Display result\n",
    "print(\"DataFrames Joined on Index:\")\n",
    "print(df_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Practical Example: Combining Larger Air Quality Datasets\n",
    "In a real-world scenario, you may have larger datasets from different air quality monitoring stations, and you want to analyze the data collectively. You can use the methods discussed above to combine these datasets into a single DataFrame for comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Air Quality Data from All Stations:\n",
      "         Date  PM2.5_Station1  PM2.5_Station2  PM2.5_Station3\n",
      "0  2024-08-01            55.0             NaN            45.0\n",
      "1  2024-08-02            60.0             NaN            52.0\n",
      "2  2024-08-03            65.0             NaN            59.0\n",
      "3  2024-08-04            70.0             NaN            66.0\n",
      "4  2024-08-05            75.0            50.0            73.0\n",
      "5  2024-08-06            80.0            58.0            80.0\n",
      "6  2024-08-07            85.0            62.0            87.0\n",
      "7  2024-08-08            90.0            68.0            94.0\n",
      "8  2024-08-09            95.0            72.0           101.0\n",
      "9  2024-08-10           100.0            76.0           108.0\n",
      "10 2024-08-11           105.0            80.0           115.0\n",
      "11 2024-08-12           110.0            84.0           122.0\n",
      "12 2024-08-13           115.0            88.0           129.0\n",
      "13 2024-08-14           120.0            92.0           136.0\n",
      "14 2024-08-15           125.0            96.0           143.0\n",
      "15 2024-08-16             NaN           100.0             NaN\n",
      "16 2024-08-17             NaN           104.0             NaN\n",
      "17 2024-08-18             NaN           108.0             NaN\n",
      "18 2024-08-19             NaN           112.0             NaN\n"
     ]
    }
   ],
   "source": [
    "# Larger datasets from multiple stations\n",
    "data_station1 = {\n",
    "    'Date': pd.date_range(start='2024-08-01', periods=15, freq='D'),\n",
    "    'PM2.5_Station1': [55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125]\n",
    "}\n",
    "\n",
    "data_station2 = {\n",
    "    'Date': pd.date_range(start='2024-08-05', periods=15, freq='D'),\n",
    "    'PM2.5_Station2': [50, 58, 62, 68, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112]\n",
    "}\n",
    "\n",
    "data_station3 = {\n",
    "    'Date': pd.date_range(start='2024-08-01', periods=15, freq='D'),\n",
    "    'PM2.5_Station3': [45, 52, 59, 66, 73, 80, 87, 94, 101, 108, 115, 122, 129, 136, 143]\n",
    "}\n",
    "\n",
    "df_station1 = pd.DataFrame(data_station1)\n",
    "df_station2 = pd.DataFrame(data_station2)\n",
    "df_station3 = pd.DataFrame(data_station3)\n",
    "\n",
    "# Merge all DataFrames on the 'Date' column\n",
    "df_merged = pd.merge(df_station1, df_station2, on='Date', how='outer')\n",
    "df_merged = pd.merge(df_merged, df_station3, on='Date', how='outer')\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Combined Air Quality Data from All Stations:\")\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Topic 1: Introduction to Data Wrangling with Pandas\n",
    "\n",
    "- **Data Wrangling:**\n",
    "  - The process of cleaning, transforming, and organizing raw data into a suitable format for analysis.\n",
    "  - Ensures data consistency, accuracy, and readiness for analysis.\n",
    "\n",
    "- **Importance of Data Wrangling:**\n",
    "  - **Improves Data Quality:** Eliminates errors, inconsistencies, and duplicates.\n",
    "  - **Facilitates Analysis:** Prepares data for efficient and effective analysis.\n",
    "  - **Enables Better Decision Making:** Leads to more reliable insights and informed decisions.\n",
    "\n",
    "- **Common Tasks in Data Wrangling:**\n",
    "  - Handling missing data.\n",
    "  - Removing duplicates.\n",
    "  - Converting data types.\n",
    "  - Merging and joining datasets.\n",
    "\n",
    "#### Topic 2: Advanced Pandas Techniques\n",
    "\n",
    "- **Handling Missing Data:**\n",
    "  - Use `fillna()` to fill missing values with a specified value or method (e.g., forward fill, backward fill).\n",
    "  - Methods: `ffill`, `bfill`, custom values, or using `limit` to control the number of fills.\n",
    "\n",
    "- **Removing Duplicates:**\n",
    "  - Use `drop_duplicates()` to remove duplicate rows from a DataFrame.\n",
    "  - Can be applied to all columns or a subset of columns.\n",
    "  - Helps in ensuring data integrity and preventing bias in analysis.\n",
    "\n",
    "- **Converting Data Types:**\n",
    "  - Use `astype()` for converting data types of columns (e.g., from string to numeric).\n",
    "  - Handling conversion errors using `errors='coerce'` to convert invalid entries to NaN.\n",
    "  - Ensuring that date columns are correctly parsed using `pd.to_datetime()`.\n",
    "\n",
    "#### Topic 3: Merging and Joining DataFrames\n",
    "\n",
    "- **Concatenation (`pd.concat()`):**\n",
    "  - Vertically stack rows or horizontally add columns of DataFrames.\n",
    "  - Useful when combining data that shares the same columns (for vertical) or the same rows (for horizontal).\n",
    "\n",
    "- **Merging (`pd.merge()`):**\n",
    "  - Combines DataFrames based on common columns or indices, similar to SQL joins.\n",
    "  - Types of joins:\n",
    "    - **Inner Join:** Includes only rows with matching keys in both DataFrames.\n",
    "    - **Outer Join:** Includes all rows, filling with NaN where data is missing.\n",
    "    - **Left Join:** Includes all rows from the left DataFrame and matching rows from the right.\n",
    "    - **Right Join:** Includes all rows from the right DataFrame and matching rows from the left.\n",
    "\n",
    "- **Joining on Indices (`DataFrame.join()`):**\n",
    "  - Similar to `merge()`, but typically used when joining DataFrames on their indices.\n",
    "  - Efficient for combining DataFrames that share the same index structure.\n",
    "\n",
    "- **Practical Applications:**\n",
    "  - Use merging and joining to integrate data from multiple sources for a comprehensive analysis.\n",
    "  - Essential for real-world data tasks like combining air quality data from multiple stations over time.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- Mastering data wrangling and advanced Pandas techniques is crucial for effective data analysis.\n",
    "- Understanding how to clean, manipulate, and combine datasets allows for more accurate, meaningful, and insightful analyses.\n",
    "- These skills form the foundation for more advanced data science tasks, enabling you to tackle real-world data challenges with confidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
